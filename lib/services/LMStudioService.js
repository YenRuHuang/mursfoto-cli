const chalk = require('chalk')
const ora = require('ora')

/**
 * ğŸ¨ LM Studio æœå‹™ - æœ¬åœ° GPU åŠ é€Ÿ AI æ¨¡å‹æœå‹™
 * æ”¯æ´ unsloth/gpt-oss-20b-GGUF ç­‰ GGUF æ ¼å¼æ¨¡å‹
 */
class LMStudioService {
  constructor (options = {}) {
    this.apiEndpoint = options.apiEndpoint || 'http://127.0.0.1:1234'
    this.modelName = options.modelName || 'unsloth/gpt-oss-20b-GGUF'
    this.defaultTimeout = options.timeout || 60000 // 60ç§’è¶…æ™‚
    this.maxRetries = options.maxRetries || 2

    // æ€§èƒ½çµ±è¨ˆ
    this.stats = {
      totalRequests: 0,
      successfulRequests: 0,
      failedRequests: 0,
      averageResponseTime: 0,
      totalTokensGenerated: 0
    }

    // å¥åº·ç‹€æ…‹
    this.lastHealthCheck = null
    this.isHealthy = false
  }

  /**
   * ğŸš€ ä¸»è¦ç”Ÿæˆæ–¹æ³•
   * @param {string} prompt - æç¤ºè©
   * @param {Object} options - ç”Ÿæˆé¸é …
   * @returns {Object} ç”Ÿæˆçµæœ
   */
  async generate (prompt, options = {}) {
    const startTime = Date.now()
    const spinner = ora('ğŸ¨ LM Studio GPU åŠ é€Ÿè™•ç†ä¸­...').start()

    this.stats.totalRequests++

    try {
      // æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹
      if (!(await this.healthCheck())) {
        throw new Error('LM Studio æœå‹™ä¸å¯ç”¨ï¼Œè«‹ç¢ºèªæ¨¡å‹å·²è¼‰å…¥')
      }

      // æ ¼å¼åŒ–æç¤ºè©
      const formattedPrompt = this.formatPrompt(prompt, options)

      // èª¿ç”¨ LM Studio API
      const result = await this.callLMStudioAPI(formattedPrompt, options)

      // æ›´æ–°çµ±è¨ˆ
      const responseTime = Date.now() - startTime
      this.stats.successfulRequests++
      this.stats.averageResponseTime = this.updateAverageTime(
        this.stats.averageResponseTime,
        this.stats.successfulRequests,
        responseTime
      )
      this.stats.totalTokensGenerated += this.estimateTokens(result.content)

      spinner.succeed(chalk.green(`âœ… LM Studio å®Œæˆ (${responseTime}ms)`))

      return {
        content: result.content,
        model: this.modelName,
        usage: {
          cost: 0, // æœ¬åœ°æ¨¡å‹å…è²»
          tokens: this.estimateTokens(result.content),
          responseTime
        },
        metadata: {
          method: 'lm-studio',
          gpuAccelerated: true,
          timestamp: new Date().toISOString()
        }
      }
    } catch (error) {
      this.stats.failedRequests++
      spinner.fail(chalk.red(`âŒ LM Studio å¤±æ•—: ${error.message}`))
      throw error
    }
  }

  /**
   * ğŸ”— èª¿ç”¨ LM Studio API
   * @param {string} prompt - æ ¼å¼åŒ–çš„æç¤ºè©
   * @param {Object} options - é¸é …
   */
  async callLMStudioAPI (prompt, options = {}) {
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), this.defaultTimeout)

    try {
      const response = await fetch(`${this.apiEndpoint}/v1/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: this.modelName,
          messages: [
            {
              role: 'system',
              content: options.systemPrompt || 'ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œå°ˆé–€å”åŠ©ç¨‹å¼é–‹ç™¼å’ŒæŠ€è¡“å•é¡Œã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚'
            },
            {
              role: 'user',
              content: prompt
            }
          ],
          temperature: options.temperature || 0.7,
          max_tokens: options.maxTokens || 4000,
          top_p: options.topP || 0.9,
          frequency_penalty: options.frequencyPenalty || 0,
          presence_penalty: options.presencePenalty || 0,
          stream: false
        }),
        signal: controller.signal
      })

      clearTimeout(timeoutId)

      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`LM Studio API éŒ¯èª¤ (${response.status}): ${errorText}`)
      }

      const data = await response.json()

      if (!data.choices || !data.choices[0] || !data.choices[0].message) {
        throw new Error('LM Studio API å›æ‡‰æ ¼å¼ç„¡æ•ˆ')
      }

      return {
        content: data.choices[0].message.content,
        usage: data.usage || {}
      }
    } catch (error) {
      clearTimeout(timeoutId)

      if (error.name === 'AbortError') {
        throw new Error(`LM Studio API èª¿ç”¨è¶…æ™‚ (${this.defaultTimeout}ms)`)
      }

      if (error.code === 'ECONNREFUSED') {
        throw new Error('LM Studio æœå‹™æœªé‹è¡Œï¼Œè«‹å•Ÿå‹• LM Studio ä¸¦è¼‰å…¥æ¨¡å‹')
      }

      throw error
    }
  }

  /**
   * ğŸ¥ å¥åº·æª¢æŸ¥
   */
  async healthCheck () {
    try {
      // å¦‚æœæœ€è¿‘æª¢æŸ¥éä¸”ç‹€æ…‹è‰¯å¥½ï¼Œç›´æ¥è¿”å›
      const now = Date.now()
      if (this.lastHealthCheck &&
          (now - this.lastHealthCheck) < 30000 && // 30ç§’å…§
          this.isHealthy) {
        return true
      }

      // åŸ·è¡Œå¿«é€Ÿå¥åº·æª¢æŸ¥
      const controller = new AbortController()
      const timeoutId = setTimeout(() => controller.abort(), 10000) // 10ç§’è¶…æ™‚

      const response = await fetch(`${this.apiEndpoint}/v1/models`, {
        method: 'GET',
        signal: controller.signal
      })

      clearTimeout(timeoutId)

      if (response.ok) {
        const data = await response.json()
        // æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
        this.isHealthy = data.data && data.data.length > 0
      } else {
        this.isHealthy = false
      }

      this.lastHealthCheck = now
      return this.isHealthy
    } catch (error) {
      this.isHealthy = false
      this.lastHealthCheck = Date.now()
      return false
    }
  }

  /**
   * ğŸ“ æ ¼å¼åŒ–æç¤ºè©
   * @param {string} prompt - åŸå§‹æç¤ºè©
   * @param {Object} options - é¸é …
   */
  formatPrompt (prompt, options = {}) {
    // é‡å°ç¨‹å¼ç¢¼ç”Ÿæˆä»»å‹™çš„ç‰¹æ®Šè™•ç†
    if (options.task === 'code-generation') {
      return `è«‹å”åŠ©ç”Ÿæˆé«˜å“è³ªçš„ç¨‹å¼ç¢¼ã€‚è¦æ±‚ï¼š
1. ä½¿ç”¨æœ€ä½³å¯¦è¸å’Œè¨­è¨ˆæ¨¡å¼
2. åŒ…å«é©ç•¶çš„è¨»è§£å’Œæ–‡æª”
3. è€ƒæ…®éŒ¯èª¤è™•ç†å’Œé‚Šç•Œæƒ…æ³
4. ç¢ºä¿ç¨‹å¼ç¢¼çš„å¯ç¶­è­·æ€§å’Œå¯æ“´å±•æ€§

ä»»å‹™ï¼š${prompt}`
    }

    // é‡å°æ¶æ§‹è¨­è¨ˆçš„ç‰¹æ®Šè™•ç†
    if (options.task === 'architecture-design') {
      return `è«‹å”åŠ©è¨­è¨ˆç³»çµ±æ¶æ§‹ã€‚è¦æ±‚ï¼š
1. è€ƒæ…®å¯æ“´å±•æ€§å’Œæ€§èƒ½
2. éµå¾ªå¾®æœå‹™å’Œé›²åŸç”Ÿæœ€ä½³å¯¦è¸
3. åŒ…å«å®‰å…¨æ€§è€ƒé‡
4. æä¾›æ¸…æ™°çš„æ¶æ§‹åœ–æè¿°

éœ€æ±‚ï¼š${prompt}`
    }

    return prompt
  }

  /**
   * ğŸ”„ é‡è©¦æ©Ÿåˆ¶
   * @param {Function} operation - è¦é‡è©¦çš„æ“ä½œ
   * @param {number} retries - å‰©é¤˜é‡è©¦æ¬¡æ•¸
   */
  async retry(operation, retries = this.maxRetries) {
    try {
      return await operation()
    } catch (error) {
      if (retries > 0) {
        console.warn(chalk.yellow(`âš ï¸ é‡è©¦ä¸­... (å‰©é¤˜ ${retries} æ¬¡)`))
        await this.sleep(1000) // ç­‰å¾… 1 ç§’
        return this.retry(operation, retries - 1)
      }
      throw error
    }
  }

  /**
   * ğŸ“Š ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  async getAvailableModels() {
    try {
      const response = await fetch(`${this.apiEndpoint}/v1/models`)
      
      if (!response.ok) {
        throw new Error(`ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: ${response.statusText}`)
      }
      
      const data = await response.json()
      return data.data || []
    } catch (error) {
      console.warn(chalk.yellow(`âš ï¸ ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: ${error.message}`))
      return []
    }
  }

  /**
   * ğŸ”§ åˆ‡æ›æ¨¡å‹
   * @param {string} modelName - æ¨¡å‹åç¨±
   */
  async switchModel(modelName) {
    const models = await this.getAvailableModels()
    const model = models.find(m => m.id === modelName)
    
    if (!model) {
      throw new Error(`æ¨¡å‹ ${modelName} ä¸å­˜åœ¨`)
    }
    
    this.modelName = modelName
    this.logger?.info(chalk.green(`âœ… å·²åˆ‡æ›åˆ°æ¨¡å‹: ${modelName}`))
  }

  /**
   * ğŸ“ˆ ä¼°ç®— token æ•¸é‡
   * @param {string} text - æ–‡æœ¬
   */
  estimateTokens(text) {
    // ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬çš„ token ä¼°ç®—
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length
    const englishWords = (text.match(/[a-zA-Z]+/g) || []).length
    const otherChars = text.length - chineseChars
    
    // ä¸­æ–‡å­—ç¬¦ï¼šç´„ 1.5 token/å­—ï¼Œè‹±æ–‡å–®è©ï¼šç´„ 1.3 token/è©
    return Math.ceil(chineseChars * 1.5 + englishWords * 1.3 + otherChars * 0.5)
  }

  /**
   * ğŸ“Š æ›´æ–°å¹³å‡æ™‚é–“
   */
  updateAverageTime(currentAvg, count, newTime) {
    return ((currentAvg * (count - 1)) + newTime) / count
  }

  /**
   * â±ï¸ ç¡çœ å‡½æ•¸
   */
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms))
  }

  /**
   * ğŸ“Š ç²å–æœå‹™çµ±è¨ˆ
   */
  getStats() {
    const successRate = this.stats.totalRequests > 0 
      ? ((this.stats.successfulRequests / this.stats.totalRequests) * 100).toFixed(1)
      : 0

    return {
      ...this.stats,
      successRate: `${successRate}%`,
      isHealthy: this.isHealthy,
      modelName: this.modelName,
      apiEndpoint: this.apiEndpoint,
      estimatedCostSavings: (this.stats.successfulRequests * 0.02).toFixed(2) // æ¯æ¬¡è«‹æ±‚ç¯€çœç´„ $0.02
    }
  }

  /**
   * ğŸ”„ é‡ç½®çµ±è¨ˆ
   */
  resetStats() {
    this.stats = {
      totalRequests: 0,
      successfulRequests: 0,
      failedRequests: 0,
      averageResponseTime: 0,
      totalTokensGenerated: 0
    }
  }
}

module.exports = LMStudioService
