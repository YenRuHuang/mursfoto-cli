const chalk = require('chalk')
const ora = require('ora')
const { spawn } = require('child_process')

/**
 * ğŸ§  AI æ¨¡å‹è·¯ç”±å™¨ - æ··åˆ AI æ¶æ§‹æ ¸å¿ƒ
 * æ™ºèƒ½é¸æ“‡æœ¬åœ° gpt-oss-20b æˆ– Claude APIï¼Œç¢ºä¿æœ€ä½³æ€§èƒ½å’Œå¯é æ€§
 */
class AIModelRouter {
  constructor () {
    this.claudeApiKey = process.env.ANTHROPIC_API_KEY
    this.localModelName = 'gpt-oss:20b'
    this.defaultTimeout = 30000 // 30ç§’è¶…æ™‚

    // æ€§èƒ½ç›£æ§
    this.stats = {
      totalRequests: 0,
      localModelRequests: 0,
      claudeApiRequests: 0,
      localModelErrors: 0,
      claudeApiErrors: 0,
      averageLocalTime: 0,
      averageClaudeTime: 0
    }

    // å¥åº·ç‹€æ…‹
    this.healthStatus = {
      localModel: 'unknown',
      claudeApi: 'unknown',
      lastChecked: null
    }

    this.initializeHealthCheck()
  }

  /**
   * ğŸš€ ä¸»è¦ç”Ÿæˆæ–¹æ³• - æ™ºèƒ½è·¯ç”±
   * @param {string} prompt - æç¤ºè©
   * @param {Object} options - ç”Ÿæˆé¸é …
   * @returns {Object} ç”Ÿæˆçµæœ
   */
  async generate (prompt, options = {}) {
    const startTime = Date.now()
    this.stats.totalRequests++

    // ä»»å‹™è¤‡é›œåº¦è©•ä¼°
    const complexity = this.assessComplexity(prompt, options)
    const forceClaudeApi = options.forceClaudeApi || complexity === 'high'

    let result = null
    let method = 'unknown'

    try {
      // ç­–ç•¥ 1: å„ªå…ˆå˜—è©¦æœ¬åœ°æ¨¡å‹ï¼ˆé™¤éå¼·åˆ¶ä½¿ç”¨ Claude æˆ–é«˜è¤‡é›œåº¦ä»»å‹™ï¼‰
      if (!forceClaudeApi && await this.isLocalModelHealthy()) {
        result = await this.generateWithLocalModel(prompt, options)
        method = 'local'
        this.stats.localModelRequests++
        this.stats.averageLocalTime = this.updateAverageTime(
          this.stats.averageLocalTime,
          this.stats.localModelRequests,
          Date.now() - startTime
        )
      }
    } catch (error) {
      console.warn(chalk.yellow(`âš ï¸ æœ¬åœ°æ¨¡å‹å¤±æ•—ï¼Œåˆ‡æ›åˆ° Claude API: ${error.message}`))
      this.stats.localModelErrors++
      this.healthStatus.localModel = 'error'
    }

    // ç­–ç•¥ 2: å‚™æ´ä½¿ç”¨ Claude API
    if (!result) {
      if (!this.claudeApiKey) {
        throw new Error('æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ä¸”æœªé…ç½® Claude API Key')
      }

      try {
        result = await this.generateWithClaudeApi(prompt, options)
        method = 'claude'
        this.stats.claudeApiRequests++
        this.stats.averageClaudeTime = this.updateAverageTime(
          this.stats.averageClaudeTime,
          this.stats.claudeApiRequests,
          Date.now() - startTime
        )
      } catch (error) {
        this.stats.claudeApiErrors++
        this.healthStatus.claudeApi = 'error'
        throw new Error(`æ‰€æœ‰ AI æ¨¡å‹éƒ½ä¸å¯ç”¨: ${error.message}`)
      }
    }

    return {
      ...result,
      metadata: {
        method,
        complexity,
        responseTime: Date.now() - startTime,
        timestamp: new Date().toISOString()
      }
    }
  }

  /**
   * ğŸ¤– æœ¬åœ°æ¨¡å‹ç”Ÿæˆ
   * @param {string} prompt - æç¤ºè©
   * @param {Object} options - é¸é …
   */
  async generateWithLocalModel (prompt, options = {}) {
    const spinner = ora('ğŸ  æœ¬åœ° AI æ¨¡å‹è™•ç†ä¸­...').start()

    try {
      // ä½¿ç”¨ harmony æ ¼å¼åŒ…è£æç¤ºè©
      const formattedPrompt = this.formatPromptForLocal(prompt, options)

      const result = await this.callOllamaApi(formattedPrompt, options)

      spinner.succeed(chalk.green('âœ… æœ¬åœ° AI æ¨¡å‹å®Œæˆ'))

      return {
        content: result,
        model: this.localModelName,
        usage: {
          cost: 0, // æœ¬åœ°æ¨¡å‹å…è²»
          tokens: this.estimateTokens(result)
        }
      }
    } catch (error) {
      spinner.fail(chalk.red('âŒ æœ¬åœ° AI æ¨¡å‹å¤±æ•—'))
      throw error
    }
  }

  /**
   * â˜ï¸ Claude API ç”Ÿæˆ
   * @param {string} prompt - æç¤ºè©
   * @param {Object} options - é¸é …
   */
  async generateWithClaudeApi (prompt, options = {}) {
    const spinner = ora('â˜ï¸ Claude API è™•ç†ä¸­...').start()

    try {
      const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': this.claudeApiKey,
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
          model: options.claudeModel || 'claude-3-sonnet-20241022',
          max_tokens: options.maxTokens || 4000,
          system: options.systemPrompt || 'You are a helpful AI assistant specialized in code generation.',
          messages: [
            {
              role: 'user',
              content: prompt
            }
          ]
        })
      })

      if (!response.ok) {
        throw new Error(`Claude API éŒ¯èª¤: ${response.status} ${response.statusText}`)
      }

      const data = await response.json()
      const content = data.content[0].text

      spinner.succeed(chalk.green('âœ… Claude API å®Œæˆ'))

      return {
        content,
        model: data.model,
        usage: {
          cost: this.calculateClaudeCost(data.usage),
          tokens: data.usage
        }
      }
    } catch (error) {
      spinner.fail(chalk.red('âŒ Claude API å¤±æ•—'))
      throw error
    }
  }

  /**
   * ğŸ” ä»»å‹™è¤‡é›œåº¦è©•ä¼°
   * @param {string} prompt - æç¤ºè©
   * @param {Object} options - é¸é …
   */
  assessComplexity (prompt, options = {}) {
    const highComplexityKeywords = [
      'architecture', 'æ¶æ§‹', 'system design', 'ç³»çµ±è¨­è¨ˆ',
      'microservices', 'å¾®æœå‹™', 'distributed', 'åˆ†æ•£å¼',
      'complex algorithm', 'è¤‡é›œæ¼”ç®—æ³•', 'optimization', 'å„ªåŒ–',
      'security analysis', 'å®‰å…¨åˆ†æ', 'performance tuning', 'æ•ˆèƒ½èª¿æ ¡'
    ]

    const mediumComplexityKeywords = [
      'api design', 'APIè¨­è¨ˆ', 'database schema', 'è³‡æ–™åº«çµæ§‹',
      'integration', 'æ•´åˆ', 'workflow', 'å·¥ä½œæµç¨‹',
      'refactor', 'é‡æ§‹', 'testing strategy', 'æ¸¬è©¦ç­–ç•¥'
    ]

    const promptLower = prompt.toLowerCase()

    // å¼·åˆ¶ä½¿ç”¨ Claude çš„æƒ…æ³
    if (options.forceClaudeApi || options.complexity === 'high') {
      return 'high'
    }

    // é«˜è¤‡é›œåº¦æª¢æ¸¬
    if (highComplexityKeywords.some(keyword => promptLower.includes(keyword.toLowerCase()))) {
      return 'high'
    }

    // ä¸­ç­‰è¤‡é›œåº¦æª¢æ¸¬
    if (mediumComplexityKeywords.some(keyword => promptLower.includes(keyword.toLowerCase())) ||
        prompt.length > 1000) {
      return 'medium'
    }

    return 'low'
  }

  /**
   * ğŸ¥ æœ¬åœ°æ¨¡å‹å¥åº·æª¢æŸ¥
   */
  async isLocalModelHealthy () {
    try {
      // å¦‚æœæœ€è¿‘æª¢æŸ¥éä¸”ç‹€æ…‹è‰¯å¥½ï¼Œç›´æ¥è¿”å›
      const now = Date.now()
      if (this.healthStatus.lastChecked &&
          (now - this.healthStatus.lastChecked) < 60000 && // 1åˆ†é˜å…§
          this.healthStatus.localModel === 'healthy') {
        return true
      }

      // åŸ·è¡Œå¿«é€Ÿå¥åº·æª¢æŸ¥
      const testResult = await this.callOllamaApi('Hello', { timeout: 10000 })

      this.healthStatus.localModel = testResult ? 'healthy' : 'unhealthy'
      this.healthStatus.lastChecked = now

      return this.healthStatus.localModel === 'healthy'
    } catch (error) {
      this.healthStatus.localModel = 'error'
      this.healthStatus.lastChecked = Date.now()
      return false
    }
  }

  /**
   * ğŸ”— Ollama API èª¿ç”¨ (HTTP API)
   * @param {string} prompt - æç¤ºè©
   * @param {Object} options - é¸é …
   */
  async callOllamaApi (prompt, options = {}) {
    const timeout = options.timeout || this.defaultTimeout

    try {
      const controller = new AbortController()
      const timeoutId = setTimeout(() => controller.abort(), timeout)

      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: this.localModelName,
          prompt,
          stream: false,
          options: {
            temperature: 0.7,
            top_p: 0.9,
            top_k: 40
          }
        }),
        signal: controller.signal
      })

      clearTimeout(timeoutId)

      if (!response.ok) {
        throw new Error(`Ollama HTTP API éŒ¯èª¤: ${response.status} ${response.statusText}`)
      }

      const data = await response.json()

      if (!data.response) {
        throw new Error('Ollama å›æ‡‰æ ¼å¼ç„¡æ•ˆ')
      }

      return data.response
    } catch (error) {
      if (error.name === 'AbortError') {
        throw new Error(`Ollama èª¿ç”¨è¶…æ™‚ (${timeout}ms)`)
      }

      if (error.code === 'ECONNREFUSED') {
        throw new Error('Ollama æœå‹™æœªé‹è¡Œï¼Œè«‹å•Ÿå‹• Ollama')
      }

      throw error
    }
  }

  /**
   * ğŸ§¹ æ¸…ç† Ollama è¼¸å‡º
   * @param {string} rawOutput - åŸå§‹è¼¸å‡º
   */
  cleanOllamaOutput (rawOutput) {
    // ç§»é™¤ ANSI è½‰ç¾©åºåˆ—å’Œè¼‰å…¥å‹•ç•«
    return rawOutput
      .replace(/[\u001b\u009b][[()#;?]*(?:[0-9]{1,4}(?:;[0-9]{0,4})*)?[0-9A-ORZcf-nqry=><]/g, '')
      .replace(/[â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â ]/g, '')
      .replace(/^\s*[â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â \s]+/gm, '')
      .replace(/Thinking\.\.\./g, '')
      .replace(/\.\.\.done thinking\./g, '')
      .trim()
  }

  /**
   * ğŸ“ æ ¼å¼åŒ–æœ¬åœ°æ¨¡å‹æç¤ºè©
   * @param {string} prompt - æç¤ºè©
   * @param {Object} options - é¸é …
   */
  formatPromptForLocal (prompt, options = {}) {
    const systemPrompt = options.systemPrompt ||
      'You are a helpful AI assistant specialized in code generation and development tasks.'

    // ç°¡åŒ–ç‰ˆ harmony æ ¼å¼
    return `${systemPrompt}\n\nUser: ${prompt}\n\nAssistant:`
  }

  /**
   * ğŸ§® è¨ˆç®— Claude æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
   * @param {Object} usage - ä½¿ç”¨çµ±è¨ˆ
   */
  calculateClaudeCost (usage) {
    if (!usage) return 0

    // Claude-3 Sonnet åƒ¹æ ¼ (ä¼°ç®—)
    const inputCostPer1M = 3.0 // $3.00 per 1M input tokens
    const outputCostPer1M = 15.0 // $15.00 per 1M output tokens

    const inputCost = (usage.input_tokens || 0) / 1000000 * inputCostPer1M
    const outputCost = (usage.output_tokens || 0) / 1000000 * outputCostPer1M

    return inputCost + outputCost
  }

  /**
   * ğŸ“Š ä¼°ç®— token æ•¸é‡
   * @param {string} text - æ–‡æœ¬
   */
  estimateTokens (text) {
    // ç°¡å–®ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦
    return Math.ceil(text.length / 4)
  }

  /**
   * ğŸ“ˆ æ›´æ–°å¹³å‡æ™‚é–“
   */
  updateAverageTime (currentAvg, count, newTime) {
    return ((currentAvg * (count - 1)) + newTime) / count
  }

  /**
   * ğŸ”§ åˆå§‹åŒ–å¥åº·æª¢æŸ¥
   */
  async initializeHealthCheck () {
    // å•Ÿå‹•æ™‚æª¢æŸ¥ä¸€æ¬¡
    await this.isLocalModelHealthy()

    // æ¯ 5 åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
    setInterval(async () => {
      await this.isLocalModelHealthy()
    }, 300000)
  }

  /**
   * ğŸ“Š ç²å–çµ±è¨ˆä¿¡æ¯
   */
  getStats () {
    const localSuccessRate = this.stats.localModelRequests > 0
      ? ((this.stats.localModelRequests - this.stats.localModelErrors) / this.stats.localModelRequests * 100).toFixed(1)
      : 0

    const claudeSuccessRate = this.stats.claudeApiRequests > 0
      ? ((this.stats.claudeApiRequests - this.stats.claudeApiErrors) / this.stats.claudeApiRequests * 100).toFixed(1)
      : 0

    return {
      ...this.stats,
      healthStatus: this.healthStatus,
      localSuccessRate: `${localSuccessRate}%`,
      claudeSuccessRate: `${claudeSuccessRate}%`,
      totalCostSavings: this.estimateCostSavings()
    }
  }

  /**
   * ğŸ’° ä¼°ç®—æˆæœ¬ç¯€çœ
   */
  estimateCostSavings () {
    // å‡è¨­æ¯å€‹æœ¬åœ°è«‹æ±‚å¹³å‡ç¯€çœ $0.01
    const avgSavingsPerLocalRequest = 0.01
    return (this.stats.localModelRequests * avgSavingsPerLocalRequest).toFixed(2)
  }

  /**
   * ğŸ¯ å¼·åˆ¶ä½¿ç”¨ç‰¹å®šæ¨¡å‹
   * @param {string} prompt - æç¤ºè©
   * @param {string} model - æ¨¡å‹é¡å‹ ('local' | 'claude')
   * @param {Object} options - é¸é …
   */
  async forceGenerate (prompt, model, options = {}) {
    if (model === 'local') {
      return await this.generateWithLocalModel(prompt, options)
    } else if (model === 'claude') {
      return await this.generateWithClaudeApi(prompt, options)
    } else {
      throw new Error(`ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: ${model}`)
    }
  }

  /**
   * ğŸ”„ é‡ç½®çµ±è¨ˆä¿¡æ¯
   */
  resetStats () {
    this.stats = {
      totalRequests: 0,
      localModelRequests: 0,
      claudeApiRequests: 0,
      localModelErrors: 0,
      claudeApiErrors: 0,
      averageLocalTime: 0,
      averageClaudeTime: 0
    }
  }
}

module.exports = AIModelRouter
